<HTML>
<HEAD>
<TITLE>RPI AFS and DCE/DFS File Servers.</TITLE>
</HEAD>

<BODY>
<H1>RPI AFS File Servers.</H1>
<B>March 12, 1997</B>
<HR>

<P>
<DL>
<DD><A HREF="#fileservers">AFS Fileservers.</A>
<DD><A HREF="#dfsfileservers">DCE/DFS Fileservers.</A>
<DD><A HREF="#restarts">Fileserver Restarts.</A>
<DD><A HREF="#serverbackups">Fileserver Backups.</A>
<DD><A HREF="#databasebackups">AFS Database Backups.</A>
<DD><A HREF="#replications">Replication Sites.</A>
<DD><A HREF="#updating">Updating CellServDB.</A>
<DD><A HREF="#otherdocs">Other Documents.</A>
</DL>

<P> Information Technology Services (SSS) at <A
HREF="http://www.rpi.edu/">Rensselaer Polytechnic Institute</A> (RPI)
maintains a fleet of <A
HREF="http://www.contrib.andrew.cmu.edu/~shadow/afs.html">Andrew File
System</A> (AFS) file servers for the campus.  We are also deploying a
DCE/DFS cell.  At the moment, these servers and their characteristics
and functions are:

<P>
Note to non-ITS members, you may or may not have access to some of the
linked documentation.

<H2><A NAME="fileservers">AFS Fileservers.</A></H2>

<TABLE>
<TH align=left>Server<TH align=center>IP Addr.<TH align=center>
Model<TH align=center>OS<TH align=center>Served
<TH align=center><A HREF="#restarts">Restart</A><TH align=left>Comments<TR>
<TD colspan=7><HR><TR>

<TD>aaron:
<TD>128.213.100.9
<TD>RS/950<TD>4.1.5
<TD align=right>8.28GB
<TD>Sun/4
<TD>NIS server, DNS, binary and configuration distribution.
<TR>

<TD>afs-14 (mishael):
<TD>128.213.100.35
<TD>RS/550<TD>4.1.5
<TD align=right>23.28GB
<TD>Sun/2
<TD>DNS, IBM S/370 Channel Emulator (backup capable), zone-3 replication.
<TR>

<TD>afs-15 (abednego):
<TD>128.213.100.38
<TD>RS/370<TD>4.1.5
<TD align=right>27.81GB
<TD>Sun/3
<TD>AFS database, DNS.
<TR>

<TD>asher:
<TD>128.113.100.31
<TD>RS/370<TD>4.1.5
<TD align=right>25.85GB
<TD>Sun/3
<TD>NTP server, NIS server, DNS, S/370 Channel Emulator (backups).
<TR>

<TD>jonah:
<TD>128.113.113.17
<TD>RS/570F<TD>4.1.5
<TD align=right>25.34GB
<TD>Sun/4
<TD>DNS, zone-2 replication.
<TR>

<TD>moses:
<TD>128.113.113.25
<TD>RS/570F<TD>4.1.5
<TD align=right>27.27GB
<TD>Sun/2
<TD>NTP server, DNS, zone-3 replication.
<TR>

<TD>nebuchadnezzar:
<TD>128.113.100.100
<TD>RS/570F<TD>4.1.5
<TD align=right>29.85GB
<TD>Sun/3
<TD>AFS database, NIS server, DNS.
<TR>

<TD>samson:
<TD>128.113.100.36
<TD>RS/570F<TD>4.1.5
<TD align=right>27.54GB
<TD>Sun/2
<TD>NIS server, AFS database, NIS server, DNS, zone-1 replication.
<TR>

<TD>samuel:
<TD>128.113.100.34
<TD>RS/570F<TD>4.1.5
<TD align=right>25.47GB
<TD>Sun/4
<TD>AFS database, NIS server, DNS, NTP server.
<TR>

<TD colspan=7><HR><TR>
<TD><TD><TD><TD>Total:<TD align=right colspan=2>217.27GB Served<TD><TD><TR>

</TABLE>

Note, all the fileservers run DSN resolving first against the local
<TT>/etc/hosts</TT> file, and then against BIND.  This allows them to
restart without depending on the canonical nameservers.

<H2><A NAME="dfsfileservers">DCE/DFS Fileservers.</A></H2>

The DCE/DFS cell consists of three machines.  DCE offers Cell wide
authentication and authorization that could, potentially, include
Windows 95 and Windows NT machines.  DFS offers several performance
and a feature advances over AFS.

<TABLE>
<TH align=left>Server<TH align=center>IP Addr.<TH align=center>
Model<TH align=center>OS<TH align=center>Served
<TH align=left>Comments<TR>
<TD colspan=6><HR><TR>

<TD>almond:
<TD>128.213.100.33
<TD>RS/F30<TD>4.1.5
<TD align=right>
<TD>Master server.
<TR>

<TD>filbert:
<TD>128.213.113.31
<TD>RS/370<TD>4.1.5
<TD align=right>8.34GB
<TD>replication server, DFS-&gt;AFS protocol translator.
<TR>

<TD>pecan:
<TD>128.213.98.217
<TD>SP/2 node<TD>4.1.5
<TD align=right>6.16GB
<TD>DFS-&gt;AFS protocol translator.
<TR>

<TD colspan=6><HR><TR>
<TD><TD><TD><TD>Total:<TD align=left colspan=2>14.50GB Served<TD><TD><TR>

</TABLE>

<H2><A NAME="restarts">Fileserver Restarts.</A></H2>

The fileserver processes are restarted, and volumes salvaged on a regular
basis.  This servers at least three purposes
<UL>
<LI> Prevents memory leaks in fileserver processes.  Transarc has steadily
removed these and other problems resulting from long tern process running,
but oddities do still occur.

<LI> Rotates log files.  Even if there were no memory, file, socket leaks
the log files do continue to grow as the afs server processes run.

<LI> Run the salvage (similar to an fsck, but on a per-volume basis---note,
there is an AFS fsck to fix partition-based problems) operation on our files.
Corruption does occur due to, oh, who knows.  Network errors, bad disk sectors,
interupted releases....
</UL>

The restart is controlled by the <TT>afs-daily</TT> script located
<TT>/usr/local/sbin</TT>.
This script runs daily at 4:00am and checks the file
<TT>/usr/afs/etc/DailyCron</TT> for instructions.  As configured now,
1/3 of the fileservers have their processes restarted on any given week,
once a month.  This takes place on the 2nd, 3rd or 4th Sunday of the
month as show in the table above.


<H2><A NAME="serverbackups">Fileserver Backups.</A></H2>

The <TT>afs-daily</TT> script also runs <TT>BackupVol.sh</TT> once a
day after a 2 hour delay (that is, it runs at 6:00am).

The <TT>BackupVols.sh</TT> script is located in <TT>/usr/afs/bin</TT> on each
file server.  This script reads the file <TT>/usr/afs/etc/Backup.Cron</TT> which lists
each file server and partition, and the days that backupsys should be run.

N.B.: Some partitions do not have daily <TT>vos backup</TT> runs.  These
are leased partitions, and the leaser would rather have extra space then
access to the readonly <TT>.backup</TT> volume.


<H2><A NAME="databasebackups">AFS Database Backups.</A></H2>

Occational backups are made of the AFS databases.  These are the files
normally found in <TT>/usr/afs/db</TT> on the database servers.  The
procedure for making a backup is:
<OL>
<LI> Select a non-syncsite database machine.  The command:
<PRE>
     uudebug &lt;database machine&gt; [7004|7003|7002|70021]
</PRE>
will tell you which machines are, and are not sync-sites.
<LI> Stop the database processes on the selected machine:
<PRE>
     bos shutdown &lt;host&gt; kaserver ptserver vlserver buserver -wait
</PRE>

<LI> Copy to contents of <TT>/usr/afs/db</TT> to a safe place.
<LI> Restart the database processes:
<PRE>
     bos startup &lt;host&gt; kaserver ptserver vlserver buserver
</PRE>

<LI> Make sure that the <TT>kaserver</TT> process did restart using
the <TT>bos status</TT> command.
</OL>

Usually, the database is kept in <TT>/usr/afs/db/backup</TT>, and the second
copy is in <TT>/dept/its/i/afs_db</TT>.
<P>

This process should be automated (perhaps by <TT>afs-daily</TT>.


<H2><A NAME="replications">Replication Sites.</A></H2>

There are several machines which serve as major replication sites.
They are divided into three zones as follows:

<TABLE>
<TH align=left>Server<TH align=left>Zone<TR>
<TD colspan=2><HR><TR>
<TD>samson<TD>zone 1<TR>
<TD>jonah<TD>zone 2<TR>
<TD>moses<TD>zone 3<TR>
<TD>afs-14<TD>zone 3<TR>
<TD colspan=2><HR><TR>
</TABLE>

<B>Important:</B> Do not put campus space on replication servers!
<P>
Campus is our most frequently replicated space.  By putting a campus
volume on a replication server the places it can be replicated are
limited, and you risk loosing a replication when volumes are moved
for ballancing.
<P>

At one time the zones referred to networks.  But, with all of the fileservers
on FDDI or CDDI 100 this designation no longer makes sense.  Because
AFS allows only one replication per server, however, it is useful to divide
replication sites for ease of administration.  This prevents ``lost'' replications
and balancing problems because the read/write copy of replications can
be kept on a different set of servers.
<P>

Most packages are replicated once for reliability, and, usually, a second
time for access speed.  By having 3 primary replication machines instead
of two the load for replcation access can be distributed over an additional
server.  If this is not fast enough, an fourth replication (or even a fifth)
may be added.
<P>

A few volumes, for example, |root.cell|
and |root.afs|, are required for <EM>every</EM> AFS access.  Other
volumes such as |home| and |campus| are often the next stop.  These volumes
are replicated five times.
<P>

<H2><A NAME="updating">Updating CellServDB.</A></H2>

The CellServDB file lives on each AFS client.  It contains the database
server names and addresses for each AFS cell accessible from the client.
This is updated once a month from a central CellServDB file maintained
by <A HREF="http://www.transarc.com/">Transarc</A>.  The procedure for
this update is:

<OL>
<LI> Put on your AFS admin cap.
<P>
<LI> <TT>cd /afs/.rpi.edu/service/etc</TT>
<P>
<LI> <TT>cp -p CellServDB.export CellServDB.export.BAK</TT>
<P>
<LI> <TT>/usr/vice/etc/update-cells -o CellServDB.export
         CellServDB.dce CellServDB.local CellServDB.transarc</TT>
<P>
This makes a new CellServDB.export file incorporating Transarc's CellServDB
(over AFS), and the local AFS and DCE CellServDB files.
<P>
<LI> Check the contents of CellServDB.export (especially the RPI entries).
If all is ok, then:
<P>
<TT>sudo cp -p CellServDB.export /usr/vice/etc/CellServDB</TT>
<P>
<LI> <TT>sudo /usr/vice/etc/update-cells -k -r /usr/vice/etc/CellServDB</TT>
<P>
This updates the Cell entries in <TT>/afs</TT>.
<P>
<LI> <TT>vos release service</TT>
<P>
At this point the clients will be updated during the next package/parcel run.
</OL>

<H2>Other Documents.<A NAME="otherdocs"></A></H2>
<DL>
<DT> <A HREF="fileserverdisks.html">
MTBF and Disk Replacement Proposal.</A>
<DT> <A HREF="file:///dept/its/i/docs/fs/deadfiles">Deadfile removal procedure.</A>
<DT> <A HREF="file:///dept/its/i/docs/fs/CFS.report.ps">CFS Report.</A>
<A NAME="ssamaps">
<DT> SSA Maps:
<DL>
        <DT><A HREF="ssa-1-12.gif">matisse, asher, fox219, afs-14, samson, moses</A>
        <DT><A HREF="ssa-2-12.gif">tigershark0, filbert, nebuchadnessar, afs-15</A>
        <DT><A HREF="ssa-2-3.gif">pecan, jonah, samuel</A>
</DL></A>
<DT> <A HREF="dependency.gif">Server Dependency Chart.</A>
</DL>


<HR>
<A HREF="http://www.rpi.edu/~sofkam/">Michael Sofka</A>.

</BODY>
</HTML>
